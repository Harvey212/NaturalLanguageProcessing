{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords \n",
    "import math\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "\n",
    "nltk.download(\"movie_reviews\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "###############################################\n",
    "def cross_validation(instances, labels, k, train_pred_func):\n",
    "\tgolden_labels = []\n",
    "\tpred_labels = []\n",
    "\tfor fold in range(k):\n",
    "\t\ttraining_instances = []\n",
    "\t\ttraining_labels = []\n",
    "\t\ttest_instances = []\n",
    "\t\ttest_labels = []\n",
    "\t\tfor i in range(len(instances)):\n",
    "\t\t\tif i % k == fold:\n",
    "\t\t\t\ttest_instances.append(instances[i])\n",
    "\t\t\t\ttest_labels.append(labels[i])\n",
    "\t\t\telse:\n",
    "\t\t\t\ttraining_instances.append(instances[i])\n",
    "\t\t\t\ttraining_labels.append(labels[i])\n",
    "\t\tpred_labels += train_pred_func(training_instances, training_labels, test_instances)\n",
    "\t\tgolden_labels += test_labels\n",
    "\t\t#print(pred_labels)\n",
    "\t\t#print(golden_labels)\n",
    "\tprint(\"Accuracy: %.4f\\nPrecision: %.4f\\nRecall: %.4f\\nF-score: %.4f\" % (\n",
    "\t\taccuracy_score(golden_labels, pred_labels), \n",
    "\t\tprecision_score(golden_labels, pred_labels), \n",
    "\t\trecall_score(golden_labels, pred_labels), \n",
    "\t\tf1_score(golden_labels, pred_labels)))\n",
    "\tprint(confusion_matrix(golden_labels, pred_labels))\n",
    "\n",
    "#############################################\n",
    "def train_then_predict_mynbc(training_instances, training_labels, test_instances):\n",
    "\tclf = MyNBC()\n",
    "\tclf.train(training_instances, training_labels)  #only do the statistics and dictionary set,no calculate\n",
    "\tpred = []\n",
    "\tfor test_instance in test_instances:\n",
    "\t\tpred.append(clf.predict(test_instance))  #predict every single article #output zero or 1\n",
    "\treturn pred\n",
    "\n",
    "###############################################\n",
    "def build_dataset():\n",
    "\tlabels = []\n",
    "\tinstances = []\n",
    "\tfor label in movie_reviews.categories():\n",
    "\t\tfor fileid in movie_reviews.fileids(label):\n",
    "\t\t\tinstances.append(movie_reviews.raw(fileid))\n",
    "\t\t\tif label == 'pos':\n",
    "\t\t\t\tlabels.append(1)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlabels.append(0)\n",
    "\treturn instances, labels\n",
    "################################################\n",
    "\n",
    "class MyNBC:\n",
    "\t'''Define your own feature extraction function'''\n",
    "\tdef __init__(self):\n",
    "\t\tself.k = 0.025  # Smoothing factor  #ok\n",
    "\t\tself.feature_table = set()\n",
    "\t\tself.y_counts = Counter()\n",
    "\t\tself.x_y_counts = defaultdict(lambda: Counter())\n",
    "\t\tself.num_instances = 0\n",
    "\t\tself.stopwords=set(stopwords.words('english'))\n",
    "\t\tself.stemmer = PorterStemmer()\n",
    "\n",
    "\t\t#self.avelen=0\n",
    "\n",
    "\tdef clean(self, string):\n",
    "\n",
    "\t\t# and not word.isdigit()\n",
    "\t\tstring = string.replace(\"@\",\" \")\n",
    "\t\tstring = string.replace(\"&\",\" \") \n",
    "\t\tstring = string.replace(\".\",\"\")\n",
    "\t\tstring = string.replace(\",\",\"\")\n",
    "\t\tstring = string.replace(\"!\",\"\")\n",
    "\t\tstring = string.replace(\"?\",\"\")\n",
    "\t\tstring = string.replace(\"/\",\" \") \n",
    "\t\tstring = string.replace(\"|\",\" \") \n",
    "\t\tstring = string.replace(\"\\\"\",\"\") \n",
    "\t\tstring = string.replace(\"~\",\" \") \n",
    "\t\tstring = string.replace(\"%\",\" \")\n",
    "\t\tstring = string.replace(\"*\",\" \")\n",
    "\t\tstring = string.replace(\"#\",\" \")\n",
    "\t\tstring = string.replace(\"+\",\" \")\n",
    "\t\tstring = string.replace(\"$\",\"\")\n",
    "\t\tstring = string.replace(\":\",\"\")\n",
    "\t\t#string = string.replace(\"_\",\" \")\n",
    "\t\tstring = string.replace(\"\\s+\",\" \")\n",
    "\t\tstring = string.lower()\n",
    "\n",
    "\t\twords=string.split(\" \")\n",
    "\t\tnostop=[word for word in words if word not in self.stopwords]\n",
    "\n",
    "\t\t#####################\n",
    "\t\t#nodigit=[]\n",
    "\n",
    "\t\t#for nd in nostop:\n",
    "\t\t#\tresult = ''.join(i for i in nd if not i.isdigit())\n",
    "\t\t#\tnodigit.append(result)\n",
    "\t\t#######################\n",
    "\n",
    "\t\tstring=\" \".join(nostop)\n",
    "\n",
    "\t\tstring = string.replace(\"-\",\" \") ##\n",
    "\t\tstring = string.replace(\"'\",\" \")\n",
    "\n",
    "\t\twords = word_tokenize(string)\n",
    "\t\tfinallist=[self.stemmer.stem(word) for word in words]\n",
    "\n",
    "\t\treturn finallist\n",
    "\n",
    "\n",
    "\tdef extract_features(self, instance) -> set:\n",
    "\n",
    "\t\tclean_wlist=self.clean(instance)\n",
    "\t\ttags = nltk.pos_tag(clean_wlist)\n",
    "\n",
    "\t\tfeatures = set()   #how many kinds of word   #the latter part is bigram\n",
    "\t\tfor w, t in tags:\n",
    "\t\t\t# Focus on only adjectives (J), adverbs (R), verbs (V), and nouns (N).\n",
    "\t\t\tif t[0] in {'J', 'R', 'V', 'N'}:    \n",
    "\t\t\t\tfeatures.add(w)    #do not add word that the set already has\n",
    "\n",
    "\t\t#create a bigram by the format bigram_cat_dog and put it in feature\n",
    "\t\t#ok\n",
    "\t\tfor i in range(len(clean_wlist) - 1):\n",
    "\t\t\tfeatures.add(\"bigram_%s_%s\" % (clean_wlist[i], clean_wlist[i+1]))   \n",
    "\t\t\n",
    "\t\treturn features\n",
    "\n",
    "\tdef train(self, instances, labels):\n",
    "\t\tfor instance, label in zip(instances, labels):\n",
    "\t\t\tself.y_counts[label] += 1   #there are only two labels #count how many articles in each label\n",
    "\n",
    "\t\t\t#the last part of the features is bigram\n",
    "\t\t\t#there are only two labels  #just count for every kind of word or bigrams in article\n",
    "\t\t\t#for a single article\n",
    "\t\t\t#f is one of the element in the set of 'the' article\n",
    "\t\t\t#so, overall we count the appearances of the word amomg many articles in the given label \n",
    "\t\t\t#regardless of the counts of the word in each article \n",
    "\t\t\tfor f in self.extract_features(instance):\n",
    "\t\t\t\tself.x_y_counts[f][label] += 1       \n",
    "\t\t\t\tself.feature_table.add(f)  #a set #summarizes the kind of word among articles regardless of label\n",
    "\t\tself.num_instances = len(instances)  #how many articles regardless of label\n",
    "\t\t# Reduce less frequent features with a threshold of 5 occurrences.\n",
    "\n",
    "\t\trecord=[]\n",
    "\t\tfor f in self.x_y_counts:\n",
    "\t\t\tif sum(self.x_y_counts[f].values()) < 5:\n",
    "\t\t\t\t#ok\n",
    "\t\t\t\t#didn't delete the x_y_count\n",
    "\t\t\t\trecord.append(f)\n",
    "\t\t\t\tself.feature_table.remove(f)\n",
    "\n",
    "\t\tfor i in record:\n",
    "\t\t\tself.x_y_counts.pop(i)\n",
    "\t\tprint(\"Number of features: %d\" % len(self.feature_table))\n",
    "\n",
    "\t\t#self.average_article_len(instances)\n",
    "\n",
    "\t#def average_article_len(self, instances):\n",
    "\t#\tacc_len=0\n",
    "\n",
    "\t#\tfor arc in instances:\n",
    "\t#\t\tacc_len+=len(arc)\n",
    "\n",
    "\t#\tself.avelen=acc_len/len(instances)\n",
    "\n",
    "\n",
    "\tdef smooth_prob(self, word, label):        #ok\n",
    "\t\t#for a given label, calculate the probability of the appearances of word searched\n",
    "\t\treturn (self.x_y_counts[word][label] + self.k) / (self.y_counts[label] + 2 * self.k)\n",
    "\n",
    "\tdef predict(self, instance):\n",
    "\t\ty_probs = Counter()\n",
    "\t\tfeatures = self.extract_features(instance)   #return the set of the test article  #use test data\n",
    "\t\t\n",
    "\t\t#len(instance)/self.avelen\n",
    "\t\tfactor=0.45\n",
    "\t\t\n",
    "\t\tfor y in self.y_counts:   #y is 0 or 1 because only 2 label in the counter\n",
    "\t\t\ty_probs[y] = math.log(self.y_counts[y] / self.num_instances)  #the probability of the label\n",
    "\n",
    "\t\t\t#argmax [log(P(y))+ sigma(log p(xi\\y))\n",
    "\t\t\t#didn't consider the count of the word in a single article\n",
    "\t\t\t#may only calculate idf\n",
    "\t\t\tfor word in self.feature_table:     \n",
    "\t\t\t\tprob = self.smooth_prob(word, y)\n",
    "\t\t\t\tif word in features:\n",
    "\t\t\t\t\ty_probs[y] += math.log(prob)          \n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t#terms which don't appear also provide information \n",
    "\t\t\t\t\t#may also due to the length of article\n",
    "\t\t\t\t\t#0.45\n",
    "\t\t\t\t\ty_probs[y] += (factor)*math.log(1.0 - prob) \n",
    "\t\t\t##########################################\n",
    "\t\t#choose the class with higher probability\n",
    "\t\treturn y_probs.most_common(1)[0][0]\n",
    "\n",
    "\n",
    "######################################################\n",
    "X,Y= build_dataset()\n",
    "cross_validation(X, Y, 5, train_then_predict_mynbc)\n",
    "#########################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
