{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import BertConfig\n",
    "from transformers import AdamW\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def strQ2B(s):\n",
    "\trstring=\"\"\n",
    "\tfor uchar in s:\n",
    "\t\tu_code=ord(uchar)\n",
    "\t\tif u_code==12288:\n",
    "\t\t\tu_code=32\n",
    "\t\telif 65281<=u_code<=65374:\n",
    "\t\t\tu_code-=65248\n",
    "\t\t\n",
    "\t\trstring+=chr(u_code)\n",
    "\n",
    "\treturn rstring\n",
    "\n",
    "\n",
    "def clear(TEX):\n",
    "\n",
    "\tTEX=strQ2B(TEX)\n",
    "\tTEX=TEX.lower()\n",
    "\tTEX=TEX.replace('…','.')\n",
    "\tTEX=TEX.replace('誒','耶')\n",
    "\tTEX=TEX.replace('痾','啊')\n",
    "\tTEX=TEX.replace('厠','廁')\n",
    "\tTEX=TEX.replace('擤','弄')\n",
    "\tTEX=TEX.replace('搥','捶')\n",
    "\tTEX=TEX.replace('嵗','歲')\n",
    "\tTEX=TEX.replace('曡','疊')\n",
    "\tTEX=TEX.replace('厰','廠')\n",
    "\tTEX=TEX.replace('聼','聽')\n",
    "\tTEX=TEX.replace('柺','拐')\n",
    "\n",
    "\treturn TEX\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class paragraph:\n",
    "\tdef  __init__(self,idd):\n",
    "\t\tself.article_id=idd\n",
    "\t\tself.start_position=[]\n",
    "\t\tself.end_position=[]\n",
    "\t\tself.entity_text=[]\n",
    "\t\tself.entity_type=[]\n",
    "\t\tself.para=[]\n",
    "\n",
    "\t\tself.sentences=[]\n",
    "\t\tself.endindex=[]\n",
    "\t\tself.labels=[]\n",
    "\t\tself.features=[]\n",
    "\n",
    "\t\tself.tagtonum=dict()\n",
    "\n",
    "\t\t#no tag->4800\n",
    "\n",
    "\t\tself.tagtonum['name']=1     #7\n",
    "\t\tself.tagtonum['location']=2 #7 \n",
    "\t\tself.tagtonum['time']=3     #70\n",
    "\t\tself.tagtonum['contact']=4  #1\n",
    "\t\tself.tagtonum['ID']=5       #0.5\n",
    "\t\tself.tagtonum['profession']=6#1\n",
    "\t\tself.tagtonum['family']=7    #1\n",
    "\t\tself.tagtonum['clinical_event']=8 #0.1\n",
    "\t\tself.tagtonum['organization']=9  #0.1\n",
    "\t\tself.tagtonum['education']=10  #0.1\n",
    "\t\tself.tagtonum['money']=11   #3\n",
    "\t\tself.tagtonum['med_exam']=12 #9\n",
    "\t\tself.tagtonum['others']=13 #0.1\n",
    "\n",
    "\n",
    "\tdef add_start_position(self,start):\n",
    "\t\tself.start_position.append(start)\n",
    "\tdef add_end_position(self,end):\n",
    "\t\tself.end_position.append(end)\n",
    "\tdef add_entity_text(self,word):\n",
    "\t\tself.entity_text.append(word)\n",
    "\tdef add_entity_type(self,typp):\n",
    "\t\tself.entity_type.append(typp)\n",
    "\tdef add_para(self,para):\n",
    "\t\tself.para.append(para)\n",
    "\n",
    "\tdef get_id(self):\n",
    "\t\treturn self.article_id\n",
    "\tdef get_start_position(self):\n",
    "\t\treturn self.start_position\n",
    "\tdef get_end_position(self):\n",
    "\t\treturn self.end_position\n",
    "\tdef get_entity_text(self):\n",
    "\t\treturn self.entity_text\n",
    "\tdef get_entity_type(self):\n",
    "\t\treturn self.entity_type\n",
    "\tdef get_para(self):\n",
    "\t\treturn (self.para)[0]\n",
    "\n",
    "\tdef sepsentence(self):\n",
    "\t\t\n",
    "\t\ttext=self.get_para()\n",
    "\t\tsentences=[]\n",
    "\t\tsentence=''\n",
    "\t\tfor i in text:\n",
    "\t\t\tif i!='？' and i!='。' and i!='～':  # and i!='，'\n",
    "\t\t\t\tsentence+=i\n",
    "\t\t\telse:\n",
    "\t\t\t\tsentence+=i\n",
    "\t\t\t\tif len(sentence)<400:\n",
    "\t\t\t\t\tsentences.append(sentence)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tfakesent=sentence.split('，')\n",
    "\t\t\t\t\tfor h in range(len(fakesent)-1):\n",
    "\t\t\t\t\t\tif h%2==0:\n",
    "\t\t\t\t\t\t\taccs=[]\n",
    "\t\t\t\t\t\t\taccs.append(fakesent[h])\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\taccs.append(fakesent[h])\n",
    "\t\t\t\t\t\t\tad='，'.join(accs)\n",
    "\t\t\t\t\t\t\tad+='，'\n",
    "\t\t\t\t\t\t\tsentences.append(ad)\n",
    "\t\t\t\t\tlastsent=fakesent[len(fakesent)-1]\n",
    "\n",
    "\t\t\t\t\tif len(accs)==1:\n",
    "\t\t\t\t\t\ttem=accs[0]+'，'+lastsent\n",
    "\t\t\t\t\t\tsentences.append(tem)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tsentences.append(lastsent)\n",
    "\n",
    "\t\t\t\tsentence=''\n",
    "\n",
    "\t\t#####################################\n",
    "\t\t##separate based on speaking person \n",
    "\t\t#finalsentences=[]\n",
    "\n",
    "\t\t#startcount=0\n",
    "\n",
    "\t\t#for k in sentences:\n",
    "\t\t#\tif startcount==0:\n",
    "\t\t#\t\ttempsentence=k\n",
    "\t\t#\t\tstartcount+=1\n",
    "\t\t#\telse:\n",
    "\t\t#\t\tif '：' in k:\n",
    "\t\t#\t\t\tfinalsentences.append(tempsentence)\n",
    "\t\t#\t\t\ttempsentence=''\n",
    "\t\t#\t\t\ttempsentence+=k\n",
    "\t\t#\t\telse:\n",
    "\t\t#\t\t\ttempsentence+=k\n",
    "\n",
    "\t\t#finalsentences.append(tempsentence)\n",
    "\n",
    "\t\t#self.sentences=finalsentences\n",
    "\t\t################################################\n",
    "\t\tself.sentences=sentences\n",
    "\n",
    "\tdef createSentEndIndex(self):\n",
    "\n",
    "\t\tsents=self.sentences\n",
    "\t\tendindex=[]\n",
    "\t\tacc=-1\n",
    "\t\tfor k in sents:\n",
    "\t\t\tacc=acc+(len(k))\n",
    "\t\t\tendindex.append(acc)\n",
    "\n",
    "\t\tself.endindex=endindex\n",
    "\n",
    "\tdef createLabels(self):\n",
    "\n",
    "\t\tself.sepsentence()\n",
    "\t\tself.createSentEndIndex()\n",
    "\n",
    "\t\ttargetstart=self.get_start_position()\n",
    "\t\ttargetend=self.get_end_position()\n",
    "\t\ttags=self.get_entity_type()\n",
    "\t\t\n",
    "\t\tlabels=[]\n",
    "\t\tfeatures=[]\n",
    "\t\tpth=0\n",
    "\t\tsentstart=0\n",
    "\n",
    "\t\ttokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "\n",
    "\t\tfor i in range(len(self.sentences)):\n",
    "\n",
    "\t\t\ttempsent=clear(self.sentences[i])\n",
    "\t\t\tcom=tokenizer.tokenize(tempsent)\n",
    "\n",
    "\t\t\t##\n",
    "\t\t\tif '[UNK]' in com:\n",
    "\t\t\t\tprint('hello')\n",
    "\t\t\t\tprint(com)\n",
    "\t\t\t\tprint(self.sentences[i])\n",
    "\t\t\t##\n",
    "\n",
    "\n",
    "\t\t\tfeature=tokenizer.encode(tempsent, return_tensors=\"pt\")\n",
    "\t\t\tfeatures.append(feature)\n",
    "\n",
    "\t\t\tans=[]\n",
    "\n",
    "\t\t\tfor m in com:\n",
    "\t\t\t\trr=m.replace('#','')\n",
    "\t\t\t\tans.append(rr)\n",
    "\n",
    "\t\t\tanscount=-1\n",
    "\t\t\tansend=[]  #relative\n",
    "\n",
    "\t\t\tfor d in range(len(ans)):\n",
    "\t\t\t\tanscount+=len(ans[d])\n",
    "\t\t\t\tansend.append(anscount)\n",
    "\n",
    "\n",
    "\t\t\tlabel=[0]*len(ans)\n",
    "\t\t\tsentend=self.endindex[i]   #absolute end of sententence\n",
    "\t\t\tseeNext=True\n",
    "\n",
    "\t\t\twhile seeNext:\n",
    "\t\t\t\tif pth<len(targetend):\n",
    "\t\t\t\t\tif ((int(targetend[pth])-1)<=sentend):\n",
    "\t\t\t\t\t\trelstart=int(targetstart[pth])-sentstart\n",
    "\t\t\t\t\t\trelend=(int(targetend[pth])-1)-sentstart\n",
    "\t\t\t\t\t\ttagg=tags[pth]\n",
    "\t\t\t\t\t\tlab=self.tagtonum[tagg]\n",
    "\n",
    "\n",
    "\t\t\t\t\t\tstk=0\n",
    "\t\t\t\t\t\tenk=0\n",
    "\n",
    "\t\t\t\t\t\tlimit=len(ans)\n",
    "\n",
    "\t\t\t\t\t\tsee1=True\n",
    "\n",
    "\t\t\t\t\t\twhile see1:\n",
    "\t\t\t\t\t\t\tif stk<limit:\n",
    "\t\t\t\t\t\t\t\tif ansend[stk]<relstart:  ##relstart=3 ansend[stk]=4  stk=3\n",
    "\t\t\t\t\t\t\t\t\tstk+=1\n",
    "\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\tsee1=False\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tsee1=False\n",
    "\n",
    "\n",
    "\t\t\t\t\t\tsee2=True\n",
    "\n",
    "\t\t\t\t\t\twhile see2:\n",
    "\t\t\t\t\t\t\tif enk<limit:\n",
    "\t\t\t\t\t\t\t\tif ansend[enk]<relend:  #relend=10 ansend[enk]=10 enk=5\n",
    "\t\t\t\t\t\t\t\t\tenk+=1\n",
    "\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\tsee2=False\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tsee2=False\n",
    "\n",
    "\n",
    "\t\t\t\t\t\t###############\n",
    "\t\t\t\t\t\t##example\n",
    "\t\t\t\t\t\t#民', '眾', 'line','嗨'\n",
    "\t\t\t\t\t\t#relstart=2  ansend[stk]=5 stk=2\n",
    "\t\t\t\t\t\t#relend5=5   ansend[enk]   enk=2 \n",
    "\t\t\t\t\t\t##############\n",
    "\n",
    "\t\t\t\t\t\tif stk==enk:                 \n",
    "\t\t\t\t\t\t\tlabel[stk]=lab\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tfor h in range(stk,(enk+1)):\n",
    "\t\t\t\t\t\t\t\tlabel[h]=lab\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\tpth+=1\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tseeNext=False\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tseeNext=False\n",
    "\t\t\t\n",
    "\n",
    "\t\t\tsentstart=sentend+1\n",
    "\t\t\tlabel.insert(0,0)  #CLS\n",
    "\t\t\tlabel.append(0)    #SEP\n",
    "\t\t\tlabels.append(label)\n",
    "\t\t\t\n",
    "\t\tself.labels=labels\n",
    "\t\tself.features=features\n",
    "\t\t#############################################\n",
    "\t\t##to know which sentence the word belongs to\n",
    "\t\t#whichsentence=[]\n",
    "\n",
    "\t\t#for i in range(len(targetwordpositions)):\n",
    "\t\t#\twh=int(targetwordpositions[i])\n",
    "\n",
    "\t\t#\tthcount=0\n",
    "\t\t#\tend=self.endindex[thcount]\n",
    "\n",
    "\t\t#\twhile wh>end:\n",
    "\t\t#\t\tthcount+=1\n",
    "\t\t#\t\tend=self.endindex[thcount]\n",
    "\t\n",
    "\t\t#\ttargetsent=self.sentences[thcount]\n",
    "\t\t#\twhichsentence.append(targetsent)\n",
    "\n",
    "\t\t#self.whichsentences=whichsentence\n",
    "\t\t#self.tagtonum\n",
    "\n",
    "\t\t#return self.whichsentences\n",
    "\t\t##########################################\n",
    "\tdef getlabels(self):\n",
    "\t\treturn self.labels\n",
    "\tdef getsentences(self):\n",
    "\t\treturn self.sentences\n",
    "\tdef getfeatures(self):\n",
    "\t\treturn self.features\n",
    "\n",
    "corpus=[]\n",
    "\n",
    "f = open(\"train_2.txt\", \"r\", encoding=\"utf8\")\n",
    "lines = f.readlines()\n",
    "\n",
    "start=0\n",
    "resflag=False\n",
    "paracount=0\n",
    "\n",
    "inputcount=False\n",
    "\n",
    "for line in lines:\n",
    "\tif start==0:\n",
    "\t\tstart+=1\n",
    "\t\tnewpara=paragraph(paracount)\n",
    "\t\tnewpara.add_para(line.replace(\"\\n\",''))\n",
    "\t\tparacount+=1\n",
    "\n",
    "\telse:\n",
    "\t\tif line=='\\n':\n",
    "\t\t\tresflag=False\n",
    "\n",
    "\t\tif 'article_id\tstart_position\tend_position\tentity_text\tentity_type' in line:\n",
    "\t\t\tresflag=True\n",
    "\t\tif resflag==True and ('article_id\tstart_position\tend_position\tentity_text\tentity_type' not in line):\n",
    "\t\t\tsee=line.replace(\"\\n\",'')\n",
    "\t\t\tres=see.split(\"\\t\")\n",
    "\t\t\tnewpara.add_start_position(res[1])\n",
    "\t\t\tnewpara.add_end_position(res[2])\n",
    "\t\t\tnewpara.add_entity_text(res[3])\n",
    "\t\t\tnewpara.add_entity_type(res[4])\n",
    "\t\t\n",
    "\t\tif '--------------------' in  line:\n",
    "\t\t\tcorpus.append(newpara)\n",
    "\t\t\tinputcount=True\n",
    "\n",
    "\t\tif inputcount==True and line!='\\n' and ('--------------------' not in  line):\n",
    "\t\t\tnewpara=paragraph(paracount)\n",
    "\t\t\tnewpara.add_para(line.replace(\"\\n\",''))\n",
    "\t\t\tparacount+=1\n",
    "\t\t\tinputcount=False\n",
    "\n",
    "\n",
    "class proj:\n",
    "\tdef __init__(self,cor):\n",
    "\t\tself.corpus=cor\n",
    "\n",
    "\t\tself.numlabels=14\n",
    "\t\tself.tagtonum=dict()\n",
    "\n",
    "\t\t#no tag->4800\n",
    "\n",
    "\t\tself.tagtonum[1]='name'     #7\n",
    "\t\tself.tagtonum[2]='location' #7 \n",
    "\t\tself.tagtonum[3]='time'     #70\n",
    "\t\tself.tagtonum[4]='contact'  #1\n",
    "\t\tself.tagtonum[5]='ID'       #0.5\n",
    "\t\tself.tagtonum[6]='profession'#1\n",
    "\t\tself.tagtonum[7]='family'    #1\n",
    "\t\tself.tagtonum[8]='clinical_event' #0.1\n",
    "\t\tself.tagtonum[9]='organization'  #0.1\n",
    "\t\tself.tagtonum[10]='education'  #0.1\n",
    "\t\tself.tagtonum[11]='money'  #3\n",
    "\t\tself.tagtonum[12]='med_exam'#9\n",
    "\t\tself.tagtonum[13]='others'#0.1\n",
    "\n",
    "\t\t#\n",
    "\t\t#self.tagtonum[7]='biomarker'\n",
    "\t\t#self.tagtonum[10]='special_skills'\n",
    "\t\t#self.tagtonum[11]='unique_treatment'\n",
    "\t\t#self.tagtonum[12]='account'\n",
    "\t\t#self.tagtonum[16]='belonging_mark'\n",
    "\t\t#\n",
    "\n",
    "\t\tself.model=BertForTokenClassification.from_pretrained('bert-base-chinese',num_labels=self.numlabels)\n",
    "\t\tself.tokenizer=BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "\t\tself.optimizer = AdamW(self.model.parameters(), lr=1e-5)\n",
    "\t\tself.finalres=[]\n",
    "\t\tself.weight=[]\n",
    "\n",
    "\tdef train(self):\n",
    "\t\tself.model.train()\n",
    "\t\tepoch=0\n",
    "\t\t#self.setweight()\n",
    "\n",
    "\t\tfor i in range(len(self.corpus)):\n",
    "\t\t\tcurrcorpus=self.corpus[i]\n",
    "\t\t\tcurrcorpus.createLabels()\n",
    "\t\t\txs=currcorpus.getfeatures()\n",
    "\t\t\tys=currcorpus.getlabels()\n",
    "\n",
    "\t\t\tfor g in range(len(xs)):\n",
    "\t\t\t\tfea=xs[g]\n",
    "\t\t\t\tlala=ys[g]\n",
    "\n",
    "\t\t\t\t####################\n",
    "\t\t\t\t#mask=[1]*len(lala)\n",
    "\t\t\t\t#mask[0]=0\n",
    "\t\t\t\t#mask[len(lala)-1]=0\n",
    "\t\t\t\t#,attention_mask=torch.tensor(mask).unsqueeze(0)\n",
    "\t\t\t\t##########################\n",
    "\t\t\t\t\n",
    "\t\t\t\t###########################\n",
    "\t\t\t\t#positions=[]\n",
    "\t\t\t\t#pos=0\n",
    "\t\t\t\t#for z in range(len(lala)):\n",
    "\t\t\t\t#\tif z==0:\n",
    "\t\t\t\t#\t\tprev=lala[z]\n",
    "\t\t\t\t#\t\tpositions.append(pos)\n",
    "\t\t\t\t#\tif z==1:\n",
    "\t\t\t\t#\t\tcurr=lala[z]\n",
    "\t\t\t\t#\t\tif curr==0:\n",
    "\t\t\t\t#\t\t\tpos+=1\n",
    "\t\t\t\t#\t\t\tpositions.append(pos)\n",
    "\n",
    "\t\t\t\t#\tif z>1:\n",
    "\t\t\t\t#\t\tprev=curr\n",
    "\t\t\t\t#\t\tcurr=lala[z]\n",
    "\t\t\t\t######################################\n",
    "\n",
    "\t\t\t\t#if sum(lala)!=0:\n",
    "\t\t\t\toutputs=self.model.forward(input_ids=fea, labels=torch.tensor(lala).unsqueeze(0))\n",
    "\n",
    "\t\t\t\t#\n",
    "\t\t\t\t#LOO = outputs.logits\n",
    "\t\t\t\t#fakepred = torch.argmax(LOO, dim=2) #include cls, sep\n",
    "\t\t\t\t#loss=self.calLoss(fakepred,lala)\n",
    "\t\t\t\t#\n",
    "\t\t\t\tloss = outputs.loss\n",
    "\t\t\t\t#\n",
    "\t\t\t\tself.model.zero_grad()\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\tself.optimizer.step()\n",
    "\n",
    "\t\t\t\tprint(epoch)\n",
    "\t\t\t\tepoch+=1\n",
    "\n",
    "\t\tself.model.save_pretrained('C:/Users/chrystal212/Desktop/nlpp')\n",
    "\n",
    "\tdef calLoss(self,inp,tarr):\n",
    "\t\tweight=[]\n",
    "\n",
    "\t\tfor i in range(len(tarr)):\n",
    "\t\t\tweight.append(self.weight[tarr[i]])\n",
    "\n",
    "\t\ttarr=torch.tensor(tarr)\n",
    "\n",
    "\t\tweight=torch.tensor(weight)\n",
    "\t\tweight=weight.float()\n",
    "\t\tlolo=torch.nn.MultiLabelSoftMarginLoss(weight=weight)(inp.float(),tarr.float())\n",
    "\t\tfll = Variable(lolo, requires_grad=True)\n",
    "\n",
    "\t\treturn fll\n",
    "\n",
    "\n",
    "\n",
    "\tdef load_pretrain(self):\n",
    "\t\tconfig = BertConfig.from_json_file('C:/Users/chrystal212/Desktop/nlpp/config.json')\n",
    "\t\tpath='C:/Users/chrystal212/Desktop/nlpp/'\n",
    "\t\tself.model=BertForTokenClassification.from_pretrained(pretrained_model_name_or_path=path,config=config)\n",
    "\n",
    "\t\t##\n",
    "\t\t#self.model.eval()\n",
    "\t\t#text='民眾：阿只是前天好很多。前天就算沒盜，'\n",
    "\t\t#temp=clear(text)\n",
    "\t\t#see=self.tokenizer.tokenize(temp)\n",
    "\t\t#inputs = self.tokenizer.encode(temp, return_tensors=\"pt\")\n",
    "\t\t#res=self.model(inputs)\n",
    "\t\t#outputs = res.logits\n",
    "\t\t#pred = torch.argmax(outputs, dim=2)\n",
    "\t\t#print(see)\n",
    "\t\t#print(pred)\n",
    "\t\t##\n",
    "\n",
    "\tdef evaluate(self,dat):\n",
    "\t\tself.model.eval()\n",
    "\n",
    "\t\tfor k in range(len(dat)):\n",
    "\t\t\ttd=dat[k]\n",
    "\t\t\ttd.sepsentence()\n",
    "\t\t\tsents=td.getsentences()\n",
    "\t\t\tparaindex=0\n",
    "\n",
    "\t\t\tfor j in range(len(sents)):\n",
    "\t\t\t\ttext=sents[j]\n",
    "\t\t\t\ttemp=clear(text)\n",
    "\n",
    "\t\t\t\tinputs = self.tokenizer.encode(temp, return_tensors=\"pt\")  #include cls ,sep\n",
    "\t\t\t\tref=self.tokenizer.tokenize(temp)  #no cls ,sep   #real character , may include #\n",
    "\n",
    "\t\t\t\t#\n",
    "\t\t\t\tif '[UNK]' in ref:\n",
    "\t\t\t\t\tprint('yes')\n",
    "\t\t\t\t\tprint(ref)\n",
    "\t\t\t\t\tprint(text)\n",
    "\t\t\t\t#誒  耶\n",
    "\t\t\t\t#痾  啊\n",
    "\t\t\t\t#厠  廁\n",
    "\t\t\t\t#擤  弄\n",
    "\n",
    "\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\tres=self.model(inputs)   \n",
    "\t\t\t\t\toutputs = res.logits\n",
    "\t\t\t\t\tpred = torch.argmax(outputs, dim=2) #include cls, sep\n",
    "\n",
    "\t\t\t\t\tpred=(pred.tolist())[0]\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tmark=False\n",
    "\t\t\t\t\tword=''\n",
    "\t\t\t\t\tstartindex=0 #character based\n",
    "\t\t\t\t\tendindex=0  #character based\n",
    "\n",
    "\t\t\t\t\tflag=False\n",
    "\t\t\t\t\tflag2=False\n",
    "\n",
    "\t\t\t\t\tfor t in range(len(pred)):\n",
    "\t\t\t\t\t\tif t!=0 and t!=(len(pred)-1):\n",
    "\t\t\t\t\t\t\ttar=(ref[t-1]).replace('#','')\n",
    "\t\t\t\t\t\t\tendindex+=len(tar)  #nextstart\n",
    "\t\t\t\t\t\t\tif pred[t]==0:\n",
    "\t\t\t\t\t\t\t\tif mark==True:\n",
    "\t\t\t\t\t\t\t\t\tflag=True\n",
    "\t\t\t\t\t\t\t\t\tmark=False\n",
    "\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\tstartindex=endindex\n",
    "\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tif mark==False:\n",
    "\t\t\t\t\t\t\t\t\ttagg=self.tagtonum[pred[t]]  #tagg is string\n",
    "\t\t\t\t\t\t\t\t\tword+=tar\n",
    "\t\t\t\t\t\t\t\t\tmark=True\n",
    "\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\ttemptag=self.tagtonum[pred[t]]\n",
    "\t\t\t\t\t\t\t\t\tif temptag==tagg:\n",
    "\t\t\t\t\t\t\t\t\t\tword+=tar\n",
    "\t\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\t\tflag=True\n",
    "\t\t\t\t\t\t\t\t\t\tflag2=True\n",
    "\n",
    "\t\t\t\t\t\t\tif flag==True:\n",
    "\t\t\t\t\t\t\t\t#print('yes')\n",
    "\t\t\t\t\t\t\t\twordlen=len(word)\n",
    "\t\t\t\t\t\t\t\twordend=startindex+wordlen\n",
    "\t\t\t\t\t\t\t\t\n",
    "\n",
    "\t\t\t\t\t\t\t\t#try:\n",
    "\t\t\t\t\t\t\t\trealword=''\n",
    "\t\t\t\t\t\t\t\tfor p in range(startindex,wordend):   #relative \n",
    "\t\t\t\t\t\t\t\t\trealword+=text[p]\n",
    "\n",
    "\t\t\t\t\t\t\t\trealstart=paraindex+startindex\n",
    "\t\t\t\t\t\t\t\trealend=paraindex+wordend\n",
    "\t\t\t\t\t\t\t\ttd.add_start_position(realstart)\n",
    "\t\t\t\t\t\t\t\ttd.add_end_position(realend)\n",
    "\t\t\t\t\t\t\t\ttd.add_entity_text(realword)\n",
    "\t\t\t\t\t\t\t\ttd.add_entity_type(tagg)\n",
    "\t\t\t\t\t\t\t\t#except:\n",
    "\t\t\t\t\t\t\t\t#print(text)\n",
    "\t\t\t\t\t\t\t\t#print(startindex)\n",
    "\t\t\t\t\t\t\t\t#print(wordend)\n",
    "\t\t\t\t\t\t\t\t#print(pred)\n",
    "\t\t\t\t\t\t\t\t#print(ref)\n",
    "\t\t\t\t\t\t\t\t#print(realword)\n",
    "\t\t\t\t\t\t\t\t\n",
    "\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\tflag=False\n",
    "\t\t\t\t\t\t\t\tword=''\n",
    "\n",
    "\t\t\t\t\t\t\t\tif flag2==True:\n",
    "\t\t\t\t\t\t\t\t\ttagg=self.tagtonum[pred[t]]\n",
    "\t\t\t\t\t\t\t\t\tword+=tar\n",
    "\t\t\t\t\t\t\t\t\tmark=True\n",
    "\t\t\t\t\t\t\t\t\tflag2=False\n",
    "\t\t\t\t\t\t\t\t\tstartindex=wordend\n",
    "\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\tstartindex=endindex\n",
    "\n",
    "\n",
    "\t\t\t\tparaindex+=len(text)\n",
    "\n",
    "\t\t\tprint(k)\n",
    "\t\t\tself.finalres.append(td)\n",
    "\n",
    "\t\tself.generateresult()\n",
    "\n",
    "\tdef generateresult(self):\n",
    "\t\tfinalframe=[]\n",
    "\n",
    "\t\tfor i in range(len(self.finalres)):\n",
    "\t\t\tcurrpara=self.finalres[i]\n",
    "\t\t\tcurrid=currpara.get_id()\n",
    "\t\t\tparastarts=currpara.get_start_position()\n",
    "\t\t\tparaends=currpara.get_end_position()\n",
    "\t\t\tparaEntityText=currpara.get_entity_text()\n",
    "\t\t\tparaEntityType=currpara.get_entity_type()\n",
    "\t\t\tfor j in range(len(paraEntityText)):\n",
    "\t\t\t\ttemp=[]\n",
    "\t\t\t\ttemp.append(currid)\n",
    "\t\t\t\ttemp.append(parastarts[j])\n",
    "\t\t\t\ttemp.append(paraends[j])\n",
    "\t\t\t\ttemp.append(paraEntityText[j])\n",
    "\t\t\t\ttemp.append(paraEntityType[j])\n",
    "\n",
    "\t\t\t\tfinalframe.append(temp)\n",
    "\n",
    "\t\tdf = pd.DataFrame(finalframe, columns=['article_id', 'start_position', 'end_position','entity_text','entity_type'])\n",
    "\t\n",
    "\t\tdf.to_excel('nlpproject.xlsx',index=False)\n",
    "\n",
    "\tdef setweight(self):\n",
    "\t\t\n",
    "\t\t#counts=[4000, 7, 7, 70, 1, 0.5, 1, 1, 0.1, 0.1, 0.1, 3, 9, 0.1]\n",
    "\t\tcounts=[5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\t\t##########################\n",
    "\t\t#counts=[0]*14\n",
    "\n",
    "\t\t#for i in range(len(self.corpus)):\n",
    "\t\t#\tcurrcorpus=self.corpus[i]\n",
    "\t\t#\tcurrcorpus.createLabels()\n",
    "\t\t#\tgg=currcorpus.getlabels()\n",
    "\t\t\t\n",
    "\t\t#\tfor m in gg:\n",
    "\t\t#\t\tfor n in m:\n",
    "\t\t#\t\t\tcounts[n]+=1\n",
    "\t\t#\tprint(i)\n",
    "\n",
    "\t\tsmall=min(counts)\n",
    "\n",
    "\t\tweight=[]\n",
    "\n",
    "\t\tfor k in counts:\n",
    "\t\t\tww=(small/k)*1000\n",
    "\t\t\tweight.append(ww)\n",
    "\n",
    "\t\tself.weight=weight\n",
    "\t\t#################################\n",
    "\n",
    "\n",
    "#################################################################\n",
    "mm=proj(corpus)\n",
    "mm.load_pretrain()\n",
    "#mm.train()\n",
    "\n",
    "testdata=[]\n",
    "\n",
    "f = open(\"test.txt\", \"r\", encoding=\"utf8\")\n",
    "lines = f.readlines()\n",
    "\n",
    "paracount=0\n",
    "flagg=False\n",
    "\n",
    "for line in lines:\n",
    "\tif 'article_id:' in line:\n",
    "\t\tflagg=True\n",
    "\n",
    "\tif line=='\\n':\n",
    "\t\tflagg=False\n",
    "\n",
    "\tif flagg==True and ('article_id:' not in line):\n",
    "\t\tnewpara=paragraph(paracount)\n",
    "\t\tnewpara.add_para(line.replace(\"\\n\",''))\n",
    "\t\tparacount+=1\n",
    "\n",
    "\tif '--------------------' in  line:\n",
    "\t\ttestdata.append(newpara)\n",
    "\n",
    "mm.evaluate(testdata)\n",
    "\n",
    "#position id\n",
    "#rule base\n",
    "#same word\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
