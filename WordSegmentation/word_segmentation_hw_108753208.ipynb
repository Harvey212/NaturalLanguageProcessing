{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#import jieba\n",
    "#from hanziconv.hanziconv import HanziConv\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers, metrics\n",
    "#from ckip import CkipSegmenter\n",
    "\n",
    "###########################################################\n",
    "def compare(actual_toks, pred_toks):\n",
    "\ti = 0\n",
    "\tj = 0\n",
    "\tp = 0\n",
    "\tq = 0\n",
    "\ttp = 0\n",
    "\tfp = 0\n",
    "\n",
    "\t##actual_toks, pred_toks is for one sentence\n",
    "    \n",
    "\twhile i < len(actual_toks) and j < len(pred_toks):\n",
    "\t\tif p == q:\n",
    "\t\t\t##compare by words, not characters\n",
    "\t\t\tif actual_toks[i] == pred_toks[j]:\n",
    "\t\t\t\ttp += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tfp += 1\n",
    "\n",
    "\t\t\t##p, q means pth or qth character in the sentence if combined\n",
    "\t\t\tp += len(actual_toks[i])\n",
    "\t\t\tq += len(pred_toks[j])\n",
    "\n",
    "\t\t\t##i,j means ith or jth word\n",
    "\t\t\ti += 1\n",
    "\t\t\tj += 1\n",
    "\t\telif p < q:\n",
    "\t\t\tp += len(actual_toks[i])\n",
    "\t\t\ti += 1\n",
    "\t\telse:\n",
    "\t\t\tfp += 1\n",
    "\t\t\tq += len(pred_toks[j])\n",
    "\t\t\tj += 1\n",
    "\n",
    "\t#len(actual_toks)  vs how many words in a sentence\n",
    "\treturn tp, fp, len(actual_toks)\n",
    "\n",
    "\n",
    "    \n",
    "def score(actual_sents, pred_sents):\n",
    "\ttp = 0\n",
    "\tfp = 0\n",
    "\ttotal = 0\n",
    "\n",
    "\t#actual_toks, pred_toks is for one sentence\n",
    "\tfor actual_toks, pred_toks in zip(actual_sents, pred_sents):\n",
    "\t\ttp_, fp_, total_ = compare(actual_toks, pred_toks)\n",
    "\t\ttp += tp_\n",
    "\t\tfp += fp_\n",
    "\t\ttotal += total_\n",
    "    \n",
    "\trecall = float(tp) / total\n",
    "\tprecision = float(tp) / (tp + fp)\n",
    "\tf1 = 2.0 * recall * precision / (recall + precision)\n",
    "\t\n",
    "\treturn recall, precision, f1\n",
    "\n",
    "#############################################################\n",
    "\n",
    "#use the algorithm of simplified chinese and use its sequence to restore back to traditional chinese\n",
    "#the only function that can be omitted if you do not use simplified chinese\n",
    "\n",
    "def restore(text, toks):\n",
    "\tresults = []\n",
    "\toffset = 0\n",
    "\tfor tok in toks:\n",
    "\t\tresults.append(text[offset:offset + len(tok)])\n",
    "\t\toffset += len(tok)\n",
    "\n",
    "\treturn results\n",
    "\n",
    "#################################################\n",
    "#x is list which has characters in the sentence\n",
    "\n",
    "def extract_sent_features(x):\n",
    "\tsent_features = []\n",
    "\tfor i in range(len(x)):\n",
    "\t\tsent_features.append(extract_char_features(x, i))\n",
    "\t\n",
    "\treturn sent_features\n",
    "\n",
    "##!!\n",
    "def extract_char_features(sent, position):\n",
    "\tchar_features = {}\n",
    "\n",
    "\t##{char_at_-3:zhong, char_at_-2:wen,.....,char_at_3:nan}\n",
    "\t##char_at is a relative position\n",
    "\t##return the former and latter character dict for a character\n",
    "\n",
    "\tfor i in range(-3, 4):\n",
    "\t\tif len(sent) > position + i >= 0:\n",
    "\t\t\tchar_features['char_at_%d' % i] = sent[position + i]\n",
    "\t\t\t\n",
    "\t\t\t###\n",
    "\t\t\t#result = segmenter.seg(sent[position + i])\n",
    "\t\t\t#char_features['pos_char_at_%d' % i]=result.pos[0]\n",
    "\t\t\t###\n",
    "\n",
    "\t\t\t###\n",
    "\t\t\tj=i+1\n",
    "\t\t\tif len(sent) > position + j >= 0:\n",
    "                char_features['bigram_at_%d' % i] = \"bigram_%s_%s\" % (sent[position + i], sent[position + j])\n",
    "            ###    \n",
    "\t\t\t#\tcheck1=sent[position + i]+sent[position + j]\n",
    "\t\t\t#\ttry:\n",
    "\t\t\t#\t\tresult1 = segmenter.seg(check1)\n",
    "\t\t\t#\t\tif len(result1.pos)==1:\n",
    "\t\t\t#\t\t\tchar_features['bigram_at_%d' % i] = \"bigram_%s_%s\" % (sent[position + i], sent[position + j])\n",
    "\t\t\t#\texcept:\n",
    "\t\t\t#\t\tprint(\"bigram doesn't exist in ckip\")\n",
    "\t\t\t###\n",
    "\n",
    "\t\t\t###\n",
    "\t\t\t#\tk=i+2\n",
    "\t\t\t#\tif len(sent) > position + k >= 0:\n",
    "\t\t\t#\t\tcheck2=sent[position + i]+sent[position + j]+sent[position + k]\n",
    "\t\t\t#\t\ttry:\n",
    "\t\t\t#\t\t\tresult2 = segmenter.seg(check2)\n",
    "\t\t\t#\t\t\tif len(result2.pos)==1:\n",
    "\t\t\t#\t\t\t\tchar_features['trigram_at_%d' % i] = \"trigram_%s_%s_%s\" % (sent[position + i], sent[position + j],sent[position + k])\n",
    "\t\t\t#\t\texcept:\n",
    "\t\t\t#\t\t\tprint(\"trigram doesn't exist in ckip\")\n",
    "\t\t\t###\n",
    "\n",
    "\n",
    "\n",
    "\t\n",
    "\treturn char_features\n",
    "#########################################################\n",
    "\n",
    "##words is list of words it cut based on spaces in the original data\n",
    "\n",
    "def words_to_tags(words):\n",
    "\t##tag the sequence of each character\n",
    "\ttags = []\n",
    "\n",
    "\t##each word may contain many chracters\n",
    "\t##one word ->S\n",
    "\t##more than one word->LMMMR\n",
    "\tfor word in words:\n",
    "\t\tif len(word) == 1:\n",
    "\t\t\ttags.append('S')\n",
    "\t\telse:\n",
    "\t\t\tfor i in range(len(word)):\n",
    "\t\t\t\tif i == 0:\n",
    "\t\t\t\t\ttags.append('L')\n",
    "\t\t\t\telif i == len(word) - 1:\n",
    "\t\t\t\t\ttags.append('R')\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttags.append('M')\n",
    "\treturn tags\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def segment(sent):\n",
    "\t##sent is a sentence which characters are connected\n",
    "\t##we list them into [\"zhong\",\"wen\",\"how\",\"nan\"]\n",
    "\t##extract_sent_feature:[{},{},{}]\n",
    "\t##predict_single:tags=['L','M','M','R']\n",
    "\n",
    "\ttags = crf_tagger.predict_single(extract_sent_features(list(sent)))\n",
    "\ttokens = []\n",
    "\ttok = \"\"\n",
    "\n",
    "\tfor ch, tag in zip(list(sent), tags):\n",
    "\t\t##if it encounters a 'L' or 'S', it adds the previous accumulated characters(a word) before it adds the new character \n",
    "\t\tif tag in ['S', 'L'] and tok != \"\":\n",
    "\t\t\ttokens.append(tok)\n",
    "\t\t\ttok = \"\"\n",
    "\t\t##if it is 'M' or 'R', you accumulate the character\n",
    "\t\ttok += ch\n",
    "\n",
    "\t##tokens is for returning words (put some characters together) based on your label\n",
    "\t##if there is something left, we add it\n",
    "\tif tok:\n",
    "\t\ttokens.append(tok)\n",
    "\t\n",
    "\treturn tokens\n",
    "\n",
    "###################################################################\n",
    "#def clean(string): \n",
    "#\tstring = string.replace(\"，\",\"\")\n",
    "#\tstring = string.replace(\"。\",\"\")\n",
    "#\tstring = string.replace(\"、\",\"\")\n",
    "#\tstring = string.replace(\"!\",\"\")\n",
    "#\tstring = string.replace(\"？\",\"\")\n",
    "\t\n",
    "#\tstring = string.replace(\"~\",\" \") \n",
    "#\tstring = string.replace(\"：\",\"\")\n",
    "\n",
    "#\tstring = string.replace(\"[\",\"\")\n",
    "#\tstring = string.replace(\"]\",\"\")\n",
    "\n",
    "#\tstring = string.replace(\"「\",\"\")\n",
    "#\tstring = string.replace(\"」\",\"\")\n",
    "\n",
    "#\tstring = string.replace(\"（\",\"\")\n",
    "#\tstring = string.replace(\"）\",\"\")\n",
    "#\tstring = string.replace(\"／\",\"\")\n",
    "\n",
    "#\tstring = string.replace(\"；\",\"\")\n",
    "\n",
    "#\tstring=string.strip()\n",
    "#\tstring=string.split()\n",
    "\n",
    "#\treturn string\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "#input\n",
    "remote_url = \"https://raw.githubusercontent.com/hhhuang/nlp2019fall/master/word_segmentation/\"\n",
    "r=requests.get(remote_url + \"data/as_training.utf8\", allow_redirects=True)\n",
    "open('as_training.utf8', 'wb').write(r.content)\n",
    "\n",
    "r=requests.get(remote_url + \"data/as_testing_gold.utf8\", allow_redirects=True)\n",
    "open('as_testing_gold.utf8', 'wb').write(r.content)\n",
    "\n",
    "#segmenter = CkipSegmenter()\n",
    "#################################################\n",
    "##in raw data , the split is based on spaces\n",
    "##it has spaces originally\n",
    "\n",
    "raw_train = []\n",
    "raw_test = []  ##contain many lists, each list is ['chinese word', 'chinese word','comma'...]\n",
    "with open(\"as_training.utf8\", encoding=\"utf8\") as fin:\n",
    "\tfor line in fin:\n",
    "\t\traw_train.append(line.strip().split(\"　\"))\n",
    "\t\t#clean_train=clean(line)\n",
    "\t\t#raw_train.append(clean_train)   # It is a full white space\n",
    "\n",
    "\n",
    "with open(\"as_testing_gold.utf8\", encoding=\"utf8\") as fin:\n",
    "\tfor line in fin:\n",
    "\t\traw_test.append(line.strip().split(\"　\"))\n",
    "\t\t#clean_test=clean(line)\n",
    "\t\t#raw_test.append(clean_test)   # It is a full white space\n",
    "\n",
    "##############################################################################\n",
    "#print(\"Number of sentences in the training data: %d\" % len(raw_train))\n",
    "#print(\"Number of sentences in the test data: %d\" % len(raw_test))\n",
    "\n",
    "##our goal is to cut the word correctly for a sentence\n",
    "##simplified chinese is better than traditional ones\n",
    "#print(list(jieba.cut(\"\".join(raw_test[0]))))\n",
    "#print(list(jieba.cut(HanziConv.toSimplified(\"\".join(raw_test[0])))))\n",
    "\n",
    "##use simplified chinese algo. results and restore back to traditional chinese\n",
    "#text = \"\".join(raw_test[0])\n",
    "#print(restore(text, list(jieba.cut(HanziConv.toSimplified(text)))))\n",
    "##########################################################\n",
    "\n",
    "train_X = []\n",
    "train_Y = []\n",
    "\n",
    "#test_X = []\n",
    "#test_Y = []\n",
    "\n",
    "###\n",
    "##print(sent)    #['zhongwen','hownan']   ##based on spaces it cut in the original data\n",
    "##print(\"\".join(sent))  #zhongwenhownan\n",
    "##print(list(\"\".join(sent)))  #['zhong','wen','how','nan']\n",
    "##text_x=[['zhong','wen','how','nan'],['how',bann]]\n",
    "###\n",
    "\n",
    "\n",
    "##raw_train contains many lists, each list of words has already been split originally based on spaces\n",
    "for sent in raw_train:\n",
    "\ttrain_X.append(list(\"\".join(sent)))  # Make the unsegmented sentence as a sequence of characters\n",
    "\ttrain_Y.append(words_to_tags(sent))\n",
    "\n",
    "\n",
    "##didn't use test_X, test_Y after\n",
    "#for sent in raw_test:\n",
    "#\ttest_X.append(list(\"\".join(sent)))  # Make the unsegmented sentence\n",
    "#\ttest_Y.append(words_to_tags(sent))\n",
    "\n",
    "##\n",
    "##test_x->test_y is one character to {S,L,M,R}\n",
    "##\n",
    "#print(raw_test[0])    \n",
    "#print(test_X[0])\n",
    "#print(test_Y[0])\n",
    "##\n",
    "\n",
    "############################################################################\n",
    "\n",
    "feature_X = []\n",
    "for x in train_X:\n",
    "\tfeature_X.append(extract_sent_features(x))\n",
    "\n",
    "\n",
    "##!!\n",
    "crf_tagger = sklearn_crfsuite.CRF(algorithm='lbfgs', min_freq=20, max_iterations=300, verbose=True)\n",
    "##feature_X: [[{},{},{}...],[{},{}..]] each dict is the former 3 and latter four characters for one chracter\n",
    "##train_Y: [['L','R','L','R'],['L','M','M','R']]  each list is one sentence, each label is for one character\n",
    "crf_tagger.fit(feature_X, train_Y)\n",
    "\n",
    "#print(segment(\"法國總統馬克宏已到現場勘災，初步傳出火警可能與目前聖母院的維修工程有關。\"))\n",
    "##############################\n",
    "##method1\n",
    "\n",
    "pred = []\n",
    "\n",
    "##how it cut the words in the original data  #it is a list\n",
    "actual = []\n",
    "\n",
    "##compare model prediction with your original answer\n",
    "for sent in raw_test:\n",
    "\tpred.append(segment(\"\".join(sent)))\n",
    "\tactual.append(sent)\n",
    "\n",
    "\n",
    "print(actual[0])\n",
    "print(pred[0])\n",
    "print(score(actual, pred))\n",
    "\n",
    "\n",
    "######################################################################\n",
    "#method2\n",
    "#if you use jiba api\n",
    "\n",
    "#pred = []\n",
    "#actual = []\n",
    "\n",
    "##for you to see  #can work without fout \n",
    "#fout = open(\"jieba.out\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "#for sent in raw_test:\n",
    "#\ttext = \"\".join(sent)\n",
    "#\tr = list(jieba.cut(HanziConv.toSimplified(text)))\n",
    "#\tr = restore(text, r)\n",
    "#\tfout.write(\" \".join(r) + \"\\n\")\n",
    "#\tpred.append(r)\n",
    "#\tactual.append(sent)\n",
    "\n",
    "#print(actual[0])\n",
    "#print(pred[0])\n",
    "#print(score(actual, pred))\n",
    "\n",
    "###########################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
